# -*- coding: utf-8 -*-
"""fine tuning bert on spam ham dataset.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YMQxx1fSScUNDhowk7rLskDjrtY6M3OG
"""

!pip install transformers datasets evaluate scikit-learn

!pip install datasets --upgrade

from datasets import load_dataset

dataset = load_dataset("sms_spam")
dataset = dataset.rename_column("label", "labels")  # Ensure label column is named 'labels'

dataset

dataset = dataset["train"].train_test_split(test_size=0.2, seed=42)

from datasets import concatenate_datasets
from random import choices

# First, split the original dataset
dataset = dataset["train"].train_test_split(test_size=0.2, seed=42)

# Now balance only the training set
train_set = dataset["train"]
spam = train_set.filter(lambda x: x["labels"] == 1)
ham = train_set.filter(lambda x: x["labels"] == 0)

# Upsample spam to match ham
upsampled_spam = concatenate_datasets([spam] * (len(ham) // len(spam)))
balanced_train = concatenate_datasets([ham, upsampled_spam]).shuffle(seed=42)

# Replace only the training set
dataset["train"] = balanced_train
# Keep test set untouched

dataset['train'].filter(lambda x: x["labels"] == 0)

from transformers import AutoTokenizer

checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)

def tokenize_function(example):
    return tokenizer(example["sms"], truncation=True)

tokenized_datasets = dataset.map(tokenize_function, batched=True)

tokenized_datasets.set_format("torch", columns=["input_ids", "attention_mask", "labels"])

from transformers import AutoModelForSequenceClassification

model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)

from transformers import TrainingArguments, Trainer
import evaluate

accuracy = evaluate.load("accuracy")

def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = logits.argmax(axis=-1)
    return accuracy.compute(predictions=predictions, references=labels)

training_args = TrainingArguments(
    output_dir="bert-finetuned-spam",
    eval_strategy="epoch",
    save_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    num_train_epochs=3,
    weight_decay=0.01,
    logging_dir='./logs',
    logging_steps=10,
    report_to="none",
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["test"],
    tokenizer=tokenizer,
    compute_metrics=compute_metrics,
)

trainer.train()

trainer.save_model("bert-spam-ham-final")
tokenizer.save_pretrained("bert-spam-ham-final")

from transformers import pipeline

clf = pipeline("text-classification", model="bert-spam-ham-final", tokenizer="bert-spam-ham-final")
print(clf("Hey, are we still meeting today?"))

predictions = trainer.predict(tokenized_datasets["test"])
print(predictions.predictions.shape, predictions.label_ids.shape)

import numpy as np

preds = np.argmax(predictions.predictions, axis=-1)

import evaluate

metric = evaluate.load("glue", "mrpc")
metric.compute(predictions=preds, references=predictions.label_ids)

from transformers import pipeline

clf = pipeline("text-classification", model="bert-spam-ham-final", tokenizer="bert-spam-ham-final")

print(clf("You’ve won a free boat!"))
print(clf("Hey, can we reschedule the meeting to 3 PM?"))

from collections import Counter
print(Counter(dataset["train"]["labels"]))

label_map = {
    "LABEL_1": "SPAM",
    "LABEL_0": "HAM"
}

samples = [
    "Win a FREE car now by clicking here!",
    "URGENT! You’ve won $1000. Call now.",
    "Hey, meeting moved to 4 PM.",
    "Can you send the report by EOD?"
]

for text in samples:
    result = clf(text)[0]
    print(f"{text} → {label_map[result['label']]} ({result['score']:.2f})")

